---
title: 快速入门NeRF原理
key: 2023-9-27-1
tags: 
- 三维计算机视觉
- 深度学习
- 计算机视觉
- NeRF
modify_date: 2023-9-27
author: 徐文江
show_author_profile: true
---

## 原视频[地址](https://www.bilibili.com/video/BV1o34y1P7Md/)         
<!--more-->  

--------------------------------          
### NeRF——Neural Radiance Fields（神经辐射场）的缩写，是来自2020年3月的一篇论文             
### Title：NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis              
### nerf论文：https://arxiv.org/pdf/2003.08934.pdf               
### nerf代码：https://github.com/bmild/nerf              
--------------------------------     
接下来给大家快速入门一下NFT的基本原理   
本次视频就不过多介绍相关的数学公式   
主要从框架上去分析NFT的整个实现流程   
那不是一篇来自2020年3月的一篇论文   
论文和代码链接   
我就放在左下角   
大家感兴趣的话可以去查看啊   
首先我们要先理解乐福为什么叫做   
神经影视的三维重建   
我们要先从计算机图形学领域的渲染开始讲   
它渲染方式是基于一个三维模型   
以及材质和光照的信息   
然后通过一个特定的视角   
然后将物体渲染成一个比较精美的画面   
也就是上面的rendering的过程   
而在计算机图像处理中有一个反渲染的过程   
也就是我们常常会说的三维重建   
他是希望能够通过渲染出来的图像   
然后重新得到三维模型   
一般这样的三维模型   
我们会用网格点云或者体术的方式去表现它   
这就是计算机图像处理中   
我们常常希望能够通过这个反渲染的操作   
去还原出一个三维模型   
而NERF其实也是这个三维重建的一门技术   
但它跟我们以往的三维重建的技术有所不同   
过往的三维重建的技术是直接通过图片   
然后重建出一个网格或者点云或者提速的模型   
而NT它是通过一种神经影视的方式   
去建这个三维模型   
什么叫神经影视呢   
从这一张图的话   
就能很清晰的看出这个影视体现在什么地方   
其他的影视就是将三维模型的信息   
存在了神经网络之中   
我们的输入是相机的位置   
输出是输出图片的RGB和不透明度   
阿尔法通过这样的方式   
我们只需要输入相机相关的信息   
那么它就会经过这个网络   
自动的去给我们渲染出一张   
可能从来没有见过的视角的一张图片   
这个过程其实就已经很能说明love的一个原理   
我们只需要通过N神经网络提及物的渲染方式   
通过已知视角的图片进行训练   
然后输入其他视角的参数   
从而预测出未出现视角的图片   
只是三维模型信息   
就储存在NFT的神经网络之中   
所以这是一种影视的表示方法   
而不是像过往点云体术网格   
这种显示的表示方式   
这里其实还有一个问题   
就是说我们一个NERF神经网络模型   
只能存储一个三维物体   
或者一个三维场景的信息   
现在也有一些相关的工作   
是   
希望网络模型能够同时用在多个场景   
和多个三维模型   
而目前这种最原始人的   
它是只能用在一个物体上   
也就是说每次如果我需要用的   
我都要对一个物体训练   
一个对应的那神经网络模型   
因为它一个那神经网络模型   
只能储存一个物体的信息   
然后接下来看一段作为官方的一段视频   
让我们能够更清晰的去了解loft的一些原理   
Input images of a scene   
Which in this visualization is a lego buldozer   
We optimize a volumetric   
Representation of the scene as a vector valued function   
Which is defined for any continuous   
Five decoordinate consisting of a location and view direction   
We parametrize   
This scene representation as a fully connected   
Deep network that takes each single five   
Decoordinate and outputs   
The corresponding volume density and view dependent   
Emitted rgb radiance at that location   
We can then use techniques from volume   
Rendering to composit   
These values along a camera ray to render any pixel   
This rendering is fully differentiable   
So we're able to optimize the scene   
Representation by adminimizing   
The air of rendering all camera rates from a collection of standard rgb images   
其实通过这个视频我们就能了解到   
NERF神经网络的输入其实就是相机的位置   
然后它的输出其实是一组颜色和不透明度值   
这里面的颜色和不透明度值   
是代表一种采样点上的颜色和不透明度值   
具体的细节我们之后再讲   
首先我们先从输入开始讲相机的位置   
它是包括相机的位置跟相机的方向   
相机的位置   
其实这个就比较简单   
通过一个XYZ的三维信息   
去表现相机所处在的位置   
第二是相机的方向   
相机的方向他用了一个极坐标的方式去表示   
它是通过方位角和仰角   
SA和F去表示相机的位置   
通过这两个参数就能表现相机朝向的方向   
既然有了相机的位置和相机的方向   
就能把相机的位置给表现出来   
有了相机的位置作为参数之后   
我们把它输入到神经网络之中   
他最后的目的就是把在这个相机角度下   
所拍摄到的图片的成像内容给输出出来   
所以它的输出是一组颜色和不透明度值   
通过这个方式   
我们就能将三维模型的信息   
储存在这样的一个神经网络之中   
然后这个网络只需要40兆   
就能把一个三维模型信息储存在神经网络之中   
在训练的过程中呢   
它是以一个像素点对应的一个射线跟相机位置   
作为一个训练资料   
它不是以整张图作为一个训练资料   
而是将其中的某个像素点都是单独抽离出来   
这里我们只需要了解到啊   
它是以像素点这一个层级作为训练资料   
而不是整张图作为一个成绩   
为一个训练资料   
然后一个batch里面就有很多个   
来自不同地方的像素点   
和不同地方的相机的参数作为训练资料   
然后在输出上它是输出RGB和不透明度值   
但是这样的输出   
它不是说直接得到一个像像素点的颜色值   
和不透明度值   
它是先输出一段采样点上的RGBA的值   
具体可以看下面这张图   
它是在一定的范围内取了一组采样点   
每个采样点都有RGBA的值   
在射线方向上对这些采样点进行一定的积分   
才得到像素点确切的颜色值   
你可以看作是C和D的这个过程   
这个过程是通过一个体积物渲染的方式   
所以其实我们通过NERF去渲染出来的物体   
它是会有一种看起来像雾一样的感觉   
云雾缭绕的那种状态   
通过这样的一个采样点的不停的去计算   
慢慢才将这个星级拟合出来   
但它本质上其实还是一个体积物的形状   
第二就是他在论文中有提到位置编码的信息   
位置编码其实主要是为了   
将图像中的高频信息体现出来   
那么采用的位置编码   
能够大幅度的提高图片的细节质量   
而不会受到周边像素平滑过后的影响   
我们也可以从原文中的图中可以看出   
不加位置编码和加了位置编码的明显区别   
首先最右边这张图就是不加位置编码的效果   
它的效果就会看起来特别的光滑   
它的边缘也不太清晰   
而加了位置编码的图会看起来更清晰一些   
也就是这第二张图   
NT中的位置编码   
是通过KATE的方式去将这样的信息融合起来   
那每个位置编码是有两项sin和cos来组合成   
然后在空间坐标系中   
它用十项就是20为十项的话就是10×2   
有20位来表示XY1Z的位置编码   
在相机方向用的是四项   
就是八维来表示XY1Z它的网络结构   
因为它原文中画的图   
跟实际上的代码是有所出入的   
所以我们根据他的实际代码   
修改了一下网络输出的图   
这里我也将位置编码的植入   
也放到了网络图当中   
其实整体看下来   
它是用一个简单粗暴的全连接层   
一直连   
最后得到最终的结果   
它的输入是相机的位置   
首先他不先把相机的方向作为最一开始的思路   
因为他认为物体的不透明度   
是跟相机的朝向是无关的   
所以它是先将相机的位置加上对应的位置编码   
作为输入   
经过相机的位置和位置编码进行合并   
变成63   
为每一层网络变成256位   
到第五层的时候   
通过一个残差结构   
解决梯度消失和梯度爆炸的问题   
坍塌结构将原来的相机位置和位置编码   
concrete到256   
这个维度再变回256   
为再经过三层256A的网络之后   
再输出为密度和另一个256层的结合   
之前的相机的方向和位置编码   
然后再输出成128位的输出   
最后再将这128位的输出   
输出为RGB的颜色值   
这样子它的RGB颜色值和它的阿尔法的值   
就这样输出出来了   
这里我们可以看到在代码中   
他用的batch size1024   
这个其实就是代表着1024个像素点   
而sample points就是代表着我们每一个像素对应的   
一个相机位置所发射的射线上   
所采样的采样点的个数   
这里的采样点的个数是64   
所以他就是说   
一个像素点对应的有64个采样点   
这个采样点是基于光线的射线而产生的   
那么这里的这64个采样点   
是需要如何处理呢   
其实它是一个morning render的部分   
这64个采样点   
其实就是在体积物里面的64个点   
但这64个点有些地方可能是透明的   
就相当于是空气   
它是透明的   
而有些地方它是不透明的   
这个光线已经碰到了物体   
那么它就会显示得不透明   
但是这里面会有一个问题   
就是说一个光线去穿透一个物体   
那个物体是会有前面和后面   
那么我们在计算的时候   
先遇到某一个面不透明度很高   
就像这个RTO一样   
前面有一个波峰   
它的纵坐标是代表不透明度   
它先碰到了这一个部分   
它不透明度很高   
那说明这已经是碰到物体的边缘了   
但是它后面又有一个波峰   
这个波峰就代表这是物体的背后   
但是实际上在正常的成像方式上   
我们是不应该把背后的内容成像到图片上   
因为这个物体是不透明的嘛   
所以说它是通过了一种特定的积分的方式   
主要是计算的第一个波峰所对应的   
颜色和不透明度值来产生对应的画面中的像素   
所以它通过这样的一个积分方式   
能够将这样的墙点上的每一个点的颜色值   
能能够转换为我们对应的像素点的颜色值   
RGBA好   
那么通过这样的方式   
其实它就能够把整个画面给渲染出来了   
最后就是我们只需要通过一组图片   
去训练一个网络   
这个网络就能储存对应的模型信息   
我们再输入一个不一样视角的相机参数   
就能把我们想要看到一个物体的角度   
给呈现出来   
这么讲   
其实大家应该对那就有一定的了解   
最后我们的下一机会给大家介绍一下   
instant ngp的快速使用流程   
希望大家能够通过这个工具   
快速体验到NFT的整个渲染效果   