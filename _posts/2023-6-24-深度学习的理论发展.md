
---
title: 深度学习的理论发展
key: 2023-6-24-3
tags: 
- 机器学习
- 深度学习
modify_date: 2023-6-24
author: 徐文江
show_author_profile: true
---
#### B站up[狗中赤兔](https://space.bilibili.com/77053688)最近更新的视频，看了一下挺有意思，解释如何在大规模参数的深度学习领域选择适合的模型，但是每次看up的视频就感觉到数学公式就开始加速，下次有时间再看懂吧。。    
<!--more-->     
不知道大家有没有听说过这个说法   
一旦理论界有了什么新的东西   
它一般都是非常伟大的   
所以如果理论界有了一个新的学生呢   
大家是不是可以把伟大打在公屏上   
虽然说有的人吧他在实践上是个矮子   
在理论上是个矮子   
他连身高上都是个矮子   
但是万一他非常伟大呢   
倒也并不能怪我在理论上是一个矮子   
因为深度学习的理论   
他的确是比实践要矮上那么一截的   
深度学习的发展主要都是依赖于实验的   
但是仅仅是实验层面的结构   
是并不足以让我们去充分理解   
深度学习模型的泛化能力的   
他缺乏了坚实的理论基础   
对我们来说   
深度学习的模型就是一个黑盒   
我们只知道这些模型效果很好   
比如我们知道和gb t的聊天体验非常丝滑呀   
用diffusion模型可以生成漂亮的纸片人老婆   
但是为什么我们能达到这么好的效果呢   
我们要怎么去验证   
去保障他们能得到什么好的效果呢   
像chat gbt啊   
或者stable diffusion   
这种让我们自娱自乐的模型还好说   
但是像是自动驾驶或者医疗诊断这种领域   
如果我们对模型为什么能成功   
没有一个准确的把握的话   
会有很大的风险的这种领域   
它有一点点小错   
就可能会导致致命的灾难   
所以我们这些搞泛化理论的人   
就想去找到深度学习模型   
分化误差的上学界   
我们希望能够通过泛化误差件去解释   
为什么深度学习模型的效果这么好   
于是呢   
这个困扰了整个领域多年的问题   
就落在了一位极懒馋笨于一身的小学生身上   
接下来呢我们有请这位油盐不进   
好吃懒做脸皮赛长城的小同学   
为我们带来单口相声   
深度学习的泛化理论   
我们今天要讲的故事是从2017年sl 2   
的一篇best paper开始的   
这篇文章的标题叫做understanding deep learning   
requires   
rethinking generalization   
也就是理解深度学习需要重新思考泛化   
我们把泛化定义为模型   
经过训练后在新的数据上做出准确预测的能力   
如果我们的模型在训练集上效果好   
但是在训练集以外的数据上表现不好   
这是什么   
过拟合   
也就是没有繁华   
在传统的机器学习理论里   
我们认为   
一个模型需要比参数量多得多的训练数据   
才能训练出一个有意义的拟合   
其实奥卡姆剃刀原则也表达了类似的观点   
如果两个模型在训练集上的效果同样好   
我们应该选那个参数更少的   
因为他更有可能学到了数据背后的规律   
而不是暴力   
借助了这个训练数据   
我们想想现在效果最好的这些模型   
有些能达到百亿甚至千亿个参数   
很明显   
对传统的机器学习理论来说   
这些模型过参数化了   
但是他们的效果实在是太好了   
他们表现出来的效果和机器学习理论是相悖的   
这个问题是deep learning这个领域   
大家都早就心知肚明了的一个问题   
但是大家都非常有默契的对这个问题避而不谈   
英语里有一个梗叫做elephant in the room   
大概是只存在一个像大象一样很大   
很明显的一个问题   
但是不知道为什么大家都对这个问题视而不见   
那么神经网络的泛化能力其实就是这么一只   
elephant in the room   
那么这篇文章就指出来了   
传统的机器学习理论   
那些只关注假设空间的capability   
而不考虑优化的那一套   
已经不足以去解释深度学习的方法表现了   
于是作者设计了一些实验去揭示这些现象   
首先他们用了各种手段去污染训练数据   
比如重新排序样本的像素点   
把样本替换成和原来样本同样   
均值和方差的高斯噪声   
或者是把标签替换成随机标签   
总之是坏事做尽   
结果我们发现诶无论怎么做   
神经网络都可以把训练误差学到零   
只不过随着污染的加重   
我们是可以观察到它的收敛速度会变慢的   
并且它的泛化误差也会相应增长   
那也就是说明   
神经网络的容量   
是足以去记住所有的训练样本的   
那这就很奇怪了   
他明明可以暴力过拟合的   
那为什么训练的时候他还能学到泛化能力呢   
既然我们的模型容量已经足以去暴力   
你和我们的数据集了   
那么在传统的机器学习理论里   
像vc维呀   
remer complicity啊   
这一系列基于模型容量的分化理论   
就不不足以去解释深度学习模型的泛化能力了   
这个时候我们是不是就不得不重新去思考   
深度学习模型的泛化能力的根源了   
那么既然是重新思考泛化   
我们之前是怎么去思考泛化的呢   
我们之前提到过   
从理论上去分析深度学习模型的泛化能力   
是非常难的   
是他为什么这么难呢   
在统计学习理论里   
我们有一系列衡量模型   
假设空间复杂度的工具   
比如之前提到过的vc维啊   
remarker complicity啊   
covering numbers这种   
然后我们会通过模型的复杂度来得到一个方法   
误差接   
但是这些方案误差上界是显示   
依赖于模型的大小的   
也就是说他们得到的结论是   
我们的模型容量   
或者说复杂度在比较小的情况下   
它的泛化能力是会更好的   
但是我们在实验中观测到的现象是这样的吗   
显然不是的   
我们所观测到的是深度学习的模型   
虽然明显过度参数化了   
但是他的表现是非常好的   
那么如果我们可以去证明   
深度学习模型的容量和参数的数量   
并不是直接相关的   
是不是就有办法去解释   
为什么深度学习模型这么成功   
那么现在我们来看一下   
之前的工作都是怎么去尝试分析   
神经网络的泛化性能的   
vc维是我们在机器学习里经常用于衡量模型   
假设空间复杂度的一个概念   
在2017年   
哈维等人就尝试了去证明神经网络的vc维   
上届发现神经网络的vc维和模型的参数量w   
还有模型的神经元数量u是直接相关的   
我们知道现在模型的参数量是非常大的   
这样会导致我们得到的泛化误差上界大的离谱   
而且vc维和数据分布是无关的   
他对任何的数据分布都是成立的   
所以用它对泛化能力进行分析   
是具有一定的普适性的   
所以通过vc v得到的分化无差接是比较松的   
这个结论也在一定程度上告诉我们   
从v4 维的角度去分析   
深度学习模型的泛化能力是非常难的   
那么有什么其他方法   
既能刻画假设空间的复杂度   
又能一定程度把数据集的分布也考虑进来呢   
这个时候我们可以去考虑用red mer complicity   
来分析方法   
误差界   
ramer conversity是通过计算我们假设空间的函数   
拟合随机噪声的能力   
来反映我们假设空间的丰富度   
通过remap complicity得到的方法   
无插件不再直接依赖于模型的规模了   
而是和参数的模还有网络的深度相关   
但是目前用ramer complicity得到的bd   
依然不足以去解释深度学习模型的泛化   
我们想想这种传统的方法   
是不是忽略掉了什么呢   
我们是不是只考虑到了模型的假设空间   
但是我们使用的随机梯度下降   
让我们的模型只探索了假设空间中的一小部分   
所以我们是不是应该把随机梯度下降   
也作为   
解释深度学习泛化能力的一个主要因素呢   
我们知道深度神经网络的损失函数   
可能会存在多个局部极小值   
比如我们看到的这个函数   
它有两个极小值点   
一个会更平坦一些   
一个是更尖锐一些的   
他们在训练集上的loss都是一样的   
但是他们可能具有非常不一样的方法能力   
我们可以看到一个尖锐的极小值点   
它对扰动是十分敏感的   
如果我们给这两个极小值上的数据点   
分别加上一个很小的随机噪声优   
我们可以看到这样很小的扰动   
对平坦的极小值点   
附近的点不会产生很大的影响   
但是尖锐的极小值会因为一点很小的扰动   
而对结果产生很大的改变   
这样的话在测试集上数据偏移一点点   
就能很大程度上影响模型的输出   
这个说明什么   
说明平坦的极小值点有着更好的鲁棒性   
所以普遍来说   
我们认为平坦的技巧值点的泛化能力是更好的   
那么我们是不是可以从寻找平坦的极小值点   
来入手   
对模型的泛化性能进行分析呢   
比如说在这个图像里   
我们的横坐标是假设空间   
纵坐标是经验风险   
它所表示的是每一组参数   
在我们的数据集上所达到的经验误差   
我们可以看到在w和w p的时候   
我们的模型分别达到了两个极小值点   
其中一个比较平坦   
另一个比较尖锐   
那么现在我们可以找到两个分布   
q和q撇   
它们分别是从假设空间h的概率   
测度空间中学习得到的一个关于参数的分布   
我们的每一个假设   
都是从这个分布中采样得到的   
因为w这个点附近比较平坦   
它对于扰动不会过于敏感   
所以w附近的这个分布   
q它的期望风险其实是和w的风险比较接近的   
而wp附近的点它对于扰动是比较敏感的   
于是q撇的期望风险可能就会和w撇的风险差   
距比较大一些   
那就这个我们可以去固定一个先验分布p   
来帮我们找到这个更平坦的最优解   
下面的这个泛化误差界   
是在1999年由macaster提出来的   
最早的pc bb   
它是通过度量先验分布p和后验分布q之间   
的距离来计算方法误差界的   
我们之前提到的传统机器学习里的泛化能力   
分析方式   
都是从模型的假设空间里选择一个最优的假设   
现在我们来换一个思路   
我们不去找一个单一的假设了   
我们现在去考虑参数的分布   
我们通过优化这个分布来得到一个更简的方法   
误差上界   
这样看的话   
虽然我们的上界对任何先验分布p都成立   
但是p的选择会直接影响我们上届的紧实程度   
早期的payption理论   
基本都是选择不依赖于数据的鲜艳的   
但是近期也有一些研究发现   
依赖数据的鲜艳可以得到更好的方法   
误差上界   
骑士派bass还有一个比较有意思的点   
他其实是把频率学派和贝叶斯学派的观点   
结合在了一起   
因为p a c   
它本来就是频率学派角度下的一个框架   
所以我们在做的就是在频率学派的视角   
搞贝叶斯推断   
而且和贝叶斯推断的不同的地方是   
pc pation理论中   
我们不需要假设我们的先验是正确的   
我们今天就先到这里啦   
拜拜   
