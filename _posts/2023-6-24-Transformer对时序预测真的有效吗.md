---
title: Transformer对时序预测真的有效吗
key: 2023-6-24-2
tags: 
- 机器学习
- 深度学习
- Transformer
modify_date: 2023-6-24
author: 徐文江
show_author_profile: true
---
#### 一篇很有意思的文章，探讨transformer虽然被广泛应用，但是否真的具有时序预测的优良性能[链接](https://www.bilibili.com/video/BV14j411A72V/)               
<!--more-->   
大家好   
很高兴呃   
跟大家分享   
我们在triple a23 上面的oral文章啊   
transformers effective for time series forecasting   
这篇文章主要是质疑了最近2年   
大家在transformer模型上面不断的做加法   
做一些模型上的修改   
从而来提升在时序预测上面的性能   
为了要探究transformer这个模型本身   
在该任务上是有效的   
我们尝试着做减法   
从而呢我们去做了一系列的探究   
首先时序任务   
其实广泛地存在于我们生活中的方方面面   
包括金融天气   
交通运动等领域   
它在不同领域的数据是差别非常大的   
但是呢他又有一些潜在的特征是相似的   
比如说大多数的数据   
可能都会有一些自己领域的趋势性   
或者是周期性   
那么本文的任务的话是做持续任务的预测   
给定l真的历史信息   
我们希望去预测未来的t真的信息   
在实习任务上面呢   
由于数据的特殊性   
所以用一个模型来去   
你和所有领域的数据其实是不太可能的   
因此的话最近的工作展现出了   
在三种模型上的不同的探究   
分别是rnn transformer和tcn相关的工作   
他们其实分别会在数据的特征提取上   
以及在decoder上面发挥出自己的优势   
和可能会有一些缺点   
那么transformer模型近年来   
在cv和o o p上面的性能的提升   
也给持续预测任务上一些启发   
尤其是它在长序列的特征提取   
和序列之间的语义关系建模上面   
可能会有潜在的价值   
因此的话从19年   
第一篇transformer模型成功地应用到了常识   
去预测这么一个任务   
在21年的top best paper里面   
这篇informer模型   
它非常好的将transformer模型   
并且他们提出了一个新的数据集   
来证明任务的价时   
在这篇best paper之后呢   
有一系列的工作   
他们分别从各个角度来改进transformer模型   
从而提升了它的性能   
那么随着这一系列工作   
在做加法的过程中   
我们会思考   
transformer模型   
它本身是否真的适用于持续预测任务呢   
那么本文的话   
质疑了现在transformer模型的有用性   
我们的依据是self attention这个核心的机制   
它其实是为了提取长的序列上面的matic信息   
那么它其实特征提取是没有考虑顺序的   
但是呢时序建模的话   
我们更关心的是在一系列长序列   
连续点上面的建模能力   
从无序到有序的建模   
这里面可能会产生一个目标的冲突人   
那么为什么像有这样的一个冲突   
但是逐年来的论文   
它依然能够有一系列的性能上的提升   
我们发现他们的对比的方法   
其实都是基于rnn base的工作   
那么这一系列工作天然的在长序列预测上   
就会遇到误差累积的问题   
他们可能用到了不恰当的baseline   
其次呢大家会先入为主的认为   
transformer模型在性能上已经有了   
非常大的提升了   
那么他们主要的问题可能在于   
self attention机制上面的效率问题   
所以后续的每天工作   
他们都在理论的效率上面   
提出了自己的啊改进的方案   
然后我们本文的话总结来说   
现有的transformer模型   
它们分别在数据的预处理   
然后在embedding encoder   
decoder上面提出了自己的改进   
然后这些改进其实都来自于持续   
数据本身的特殊性   
比如说他会对数据进行decomposition   
或者是它会引入持续的一些local的操作   
然后为了探究常识去预测的transformer模型   
他们是否真正的对这个任务有很好的建模能力   
我们做了一系列的empirical study来探究这个问题   
首先我们认为输入越长的序列   
其实它的输入信息越多   
它可能能达到的效果越好   
但是我们在大量的现有的benchmark上面   
去做了一些实验   
随着横轴输入数据的时间信息提升   
它的误差其实是基本上没有变化   
或者是在有些数据上   
它的误差还会变得更差   
那么说明现在的传输方面   
模型可能并没有很好的去提取到历史的信息   
甚至他们会倾向于去overfit   
到一些持续的噪声上面   
另外一个问题就是transformer模型   
尽管他加了一些positional embedding   
或者是temporal embedding   
他们也是希望去弥补持续的顺序特殊性   
但是这些模型真的能很好的保留持续的顺序嘛   
我们也做了一些实验   
比如说我们把历史真的信息进行了random   
shuffle   
或者是我们把前后半段的序列做了一个替换   
我们会发现现在的sota transformer上面   
它们的性能并没有较大的变化   
但是呢我们如果用一个非常简单的时序   
线性模型   
我们会发现它对于顺序关系是非常敏感的   
那么它会有非常大的性能的变化   
然后其次的话我们探究了在长序列预测中   
到底产生方面模型能学到什么信息   
我们发现在一个输入的历史划窗里面   
我们给他前半段的信息   
或者给他后半段的信息   
对于现在的transformer模型来说   
它们的性能几乎是没有差异的   
那说明给他的信息可能对于他来说   
顺序和先后关系可能也不是特别重要   
其次的话   
我们探究了在现有的transformer模型的   
self attention这个模块上面是否真正的起到作用   
我们以tripi 21的best paper informer为例   
我们逐步的将他的self attention layer   
替换成一个持续的线性程   
或者是把他的其他的操作都删减到   
只有一个positional embedding以及线性程   
以及最后我们把positional embellion删掉   
只留下一个线行程   
我们会发现在mc这个误差指标上面   
它们的性能   
其实在大多数的情况下都是在逐步下降   
那么在有一个最简单的线形成的结果上   
是效果最好的   
进一步呢我们在探究了大家为了保留时序信息   
所以引入了positional embedding   
还有temporal embedding这些明白的操作   
然后我们做了一定的删减   
我们会发现在现有的sota transformer上面   
大家其实不要positional bi   
往往能达到最好的效果   
这样就说明有时候做一些加法   
可能并没有带来真正的收益   
更进一步   
我们从数据方面   
比如说transformer的性能提升   
往往大家会归功于数据量的增加   
那么我们在这里   
因为时序数据它是接收到的观测值   
那么它的数据并不能无限的增加   
所以我们在这里做了一个实验室   
我们将输入数据原来是1.5年的观测数据   
我们把它缩减到1年这样一个整周期的数据   
我们会发现现有的sota transformer模型   
再给更短的时间输入   
以及更完整的一个一整年的周期的时候   
往往能达到更好的效果   
说明在当前的数据量下   
其实并不是数据越多   
结果越好   
最后的话我们探究了transformer模型   
都在性能已经达到很好的水平情况下   
那么他们往往会提出各种各样的operation   
来降低自己的理论开销   
但是我们实际在同样一个gpu上面去测试了   
他们的实际值   
可能并没有发生那么大的变化   
甚至有一些实际值会变得更差   
进而呢我们发现相对一些做加法   
或者是在transformer这个结构上   
做一些改进的工作   
当我们逐步的去做减法   
直到变成一个线性模型的时候   
它的性能居然能达到最好的效果   
那么因此我们为了要验证   
transformer模型   
是否真的在时序预测任务上是非常有效的   
我们引入了一个新的baseline   
它是一个线性模型   
然后为了要应对持续数据里面   
可能的趋势性和周期性   
以及它带来的distribution shift   
我们在这个线性模型基础上   
做了两个不同层面的数据预处理   
分别是decomposition和normalize   
那么我们把这三种线形成   
把它合成为lt s f06    
这个线性模型的话是一个非常简单的   
它就是输入l增的历史征以及预测未来的t征   
那么它只有一个weight   
就是l乘上t的weight   
然后它有非常多优势   
比如说它有最大的相对路径   
以及它的效率非常高   
以及它的权重   
我们可以做一些可视化   
同时呢我们在九个常用的榜单上面   
都做了大量的实验   
然后大多数结果都能表明   
仅用一个线性模型就能超过   
现在sota transformer   
大概20%到50%的这么一个性能   
对具体的数值   
大家可以参考我们paper里面进一步的分析   
然后为了探究我们的模型到底学到了什么   
以及现在的transformer模型学到了什么   
我们做了一些可视化   
然后大家可以发现我们的线性模型的话   
其实往往能比较好地保留这个趋势性和周期性   
但是transformer模型   
可能它的预测的尺度就会发生比较大的变化   
同时我们在汇率预测   
这样一个   
不是很具有周期性的数据上去进行了预测   
我们发现线性模型主要是能保留这个趋势性   
但是transformer模型可能会overfit到一些噪声上   
面   
产生了一些数据的波动   
最后我们总结一下这个工作   
我们为了质疑现在transformer模型   
在尝试去预测上面的有效性   
提出了一个非常简单的线性模型   
然后我们在现有的九个数据集上   
进行了大量的验证   
发现transformer模型可能并不是   
当前在解决这个任务上比较好的方案   
然后我们给出了一些future work啊   
第一个是一个novel的大规模的榜单   
可能是非常有必要的点   
然后第二个的话是   
我们可以去考虑更多的动态模型   
来处理时序数据的diribution shift   
然后第三点的话是   
我们发现我们现在的线性模型   
并没有考虑空间关系   
那么一个更好的时空建模的模型   
可能是有必要的   
最后我们的论文和我们的code   
都可以通过扫码获得   
谢谢大家   