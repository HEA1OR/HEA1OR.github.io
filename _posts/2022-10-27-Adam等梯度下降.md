---
title: Adam等梯度下降 
key: 2022-10-27
tags: 
- Adam
- SGD 
- NAG
- 机器学习
- 数学
modify_date: 2022-10-27
author: 徐文江
show_author_profile: true
---


## Adam等梯度下降      

呼~网站搭的真累，半天找到一个能部署的，结果又碰到html格式问题，今天先写个博客吧        
<!--more-->     
-------

[从 SGD 到 Adam —— 深度学习优化算法概览](https://zhuanlan.zhihu.com/p/32626442)            

过多的细节就不提了，下面把各种算法的特点总结一下        

### 朴素 SGD         

就是简单的减去反向梯度乘系数，缺点是收敛速度慢，可能在鞍点震荡         

### SGD-M          

在原步长上增加动量项，动量即此前累计的下降方向        

### NAG（Nesterov Accelerated Gradient）      

先算出动量作用后的位置，再在此位置上求梯度       

可以预测未来，提前调整更新速率         

### Adagrad         

SGD、SGD-M 和 NAG 均是以相同的学习率去更新 θ 的各个分量          

Adagrad对于更新频繁的参数步长较小，使得学习到的参数更稳定，不至于被单个样本影响太多。          

引入二阶动量，表现为对角矩阵，出现在学习率的分母        

### RMSprop         

考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度            

vt=γvt−1+(1−γ)⋅diag(gt2)       

### Adam       

是 RMSprop 和 Momentum 的结合，对一阶动量也是用指数移动平均计算        

### NAdam        

在 Adam 之上融合了 NAG 的思想。           